{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Installing dependencies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b2123d9c5f184dc"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vmas in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (1.2.11)\r\n",
      "Requirement already satisfied: numpy in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from vmas) (1.24.4)\r\n",
      "Requirement already satisfied: torch in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from vmas) (2.1.0.dev20230809)\r\n",
      "Requirement already satisfied: pyglet<=1.5.27 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from vmas) (1.5.27)\r\n",
      "Requirement already satisfied: gym in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from vmas) (0.26.2)\r\n",
      "Requirement already satisfied: six in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from vmas) (1.16.0)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from gym->vmas) (2.2.1)\r\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from gym->vmas) (0.0.8)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from gym->vmas) (6.0.0)\r\n",
      "Requirement already satisfied: filelock in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch->vmas) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch->vmas) (4.7.1)\r\n",
      "Requirement already satisfied: sympy in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch->vmas) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch->vmas) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch->vmas) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch->vmas) (2023.4.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym->vmas) (3.11.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from jinja2->torch->vmas) (2.1.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from sympy->torch->vmas) (1.3.0)\r\n",
      "Requirement already satisfied: Pillow in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (10.0.0)\r\n",
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cpu\r\n",
      "Requirement already satisfied: torch in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (2.1.0.dev20230809)\r\n",
      "Requirement already satisfied: torchvision in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (0.16.0.dev20230809)\r\n",
      "Requirement already satisfied: torchaudio in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (2.1.0.dev20230809)\r\n",
      "Requirement already satisfied: filelock in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch) (4.7.1)\r\n",
      "Requirement already satisfied: sympy in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch) (2023.4.0)\r\n",
      "Requirement already satisfied: numpy in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torchvision) (1.24.4)\r\n",
      "Requirement already satisfied: requests in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torchvision) (2.31.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torchvision) (10.0.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from jinja2->torch) (2.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests->torchvision) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests->torchvision) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests->torchvision) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests->torchvision) (2023.7.22)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: ipython in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (8.12.2)\r\n",
      "Requirement already satisfied: backcall in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (0.2.0)\r\n",
      "Requirement already satisfied: decorator in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (5.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (0.18.1)\r\n",
      "Requirement already satisfied: matplotlib-inline in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (0.1.6)\r\n",
      "Requirement already satisfied: pickleshare in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (0.7.5)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (3.0.36)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (2.15.1)\r\n",
      "Requirement already satisfied: stack-data in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (0.2.0)\r\n",
      "Requirement already satisfied: traitlets>=5 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (5.7.1)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (4.7.1)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (4.8.0)\r\n",
      "Requirement already satisfied: appnope in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (0.1.2)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from jedi>=0.16->ipython) (0.8.3)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from pexpect>4.3->ipython) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython) (0.2.5)\r\n",
      "Requirement already satisfied: executing in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from stack-data->ipython) (0.8.3)\r\n",
      "Requirement already satisfied: asttokens in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from stack-data->ipython) (2.0.5)\r\n",
      "Requirement already satisfied: pure-eval in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from stack-data->ipython) (0.2.2)\r\n",
      "Requirement already satisfied: six in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from asttokens->stack-data->ipython) (1.16.0)\r\n",
      "Requirement already satisfied: autoreload in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (0.1.2)\r\n",
      "Requirement already satisfied: selenium in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from autoreload) (4.11.2)\r\n",
      "Requirement already satisfied: click in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from autoreload) (8.1.6)\r\n",
      "Requirement already satisfied: watchdog in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from autoreload) (3.0.0)\r\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from selenium->autoreload) (1.26.16)\r\n",
      "Requirement already satisfied: trio~=0.17 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from selenium->autoreload) (0.22.2)\r\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from selenium->autoreload) (0.10.3)\r\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from selenium->autoreload) (2023.7.22)\r\n",
      "Requirement already satisfied: attrs>=20.1.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from trio~=0.17->selenium->autoreload) (22.1.0)\r\n",
      "Requirement already satisfied: sortedcontainers in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from trio~=0.17->selenium->autoreload) (2.4.0)\r\n",
      "Requirement already satisfied: idna in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from trio~=0.17->selenium->autoreload) (3.4)\r\n",
      "Requirement already satisfied: outcome in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from trio~=0.17->selenium->autoreload) (1.2.0)\r\n",
      "Requirement already satisfied: sniffio in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from trio~=0.17->selenium->autoreload) (1.2.0)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from trio~=0.17->selenium->autoreload) (1.1.3)\r\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from trio-websocket~=0.9->selenium->autoreload) (1.2.0)\r\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from urllib3[socks]<3,>=1.26->selenium->autoreload) (1.7.1)\r\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->autoreload) (0.14.0)\r\n",
      "Requirement already satisfied: torch-geometric in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (2.3.1)\r\n",
      "Requirement already satisfied: tqdm in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch-geometric) (4.66.1)\r\n",
      "Requirement already satisfied: numpy in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch-geometric) (1.24.4)\r\n",
      "Requirement already satisfied: scipy in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch-geometric) (1.10.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch-geometric) (3.1.2)\r\n",
      "Requirement already satisfied: requests in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch-geometric) (2.31.0)\r\n",
      "Requirement already satisfied: pyparsing in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch-geometric) (3.1.1)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch-geometric) (1.3.0)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch-geometric) (5.9.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from jinja2->torch-geometric) (2.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests->torch-geometric) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests->torch-geometric) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests->torch-geometric) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests->torch-geometric) (2023.7.22)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from scikit-learn->torch-geometric) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from scikit-learn->torch-geometric) (3.2.0)\r\n",
      "Collecting wandb\r\n",
      "  Obtaining dependency information for wandb from https://files.pythonhosted.org/packages/ed/d7/8927aef63869d5d379adb63dc97f9cbc53830fdf85457b84a156fabcb231/wandb-0.15.8-py3-none-any.whl.metadata\r\n",
      "  Downloading wandb-0.15.8-py3-none-any.whl.metadata (8.3 kB)\r\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from wandb) (8.1.6)\r\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\r\n",
      "  Obtaining dependency information for GitPython!=3.1.29,>=1.0.0 from https://files.pythonhosted.org/packages/67/50/742c2fb60989b76ccf7302c7b1d9e26505d7054c24f08cc7ec187faaaea7/GitPython-3.1.32-py3-none-any.whl.metadata\r\n",
      "  Downloading GitPython-3.1.32-py3-none-any.whl.metadata (10.0 kB)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from wandb) (2.31.0)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from wandb) (5.9.0)\r\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\r\n",
      "  Obtaining dependency information for sentry-sdk>=1.0.0 from https://files.pythonhosted.org/packages/86/bb/ecb87fd214d5bbade07edf2ecdd829cf346e5b552689d6228112c6517286/sentry_sdk-1.29.2-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl.metadata (8.8 kB)\r\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\r\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\r\n",
      "Requirement already satisfied: PyYAML in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from wandb) (6.0)\r\n",
      "Collecting pathtools (from wandb)\r\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting setproctitle (from wandb)\r\n",
      "  Downloading setproctitle-1.3.2-cp38-cp38-macosx_10_9_universal2.whl (16 kB)\r\n",
      "Requirement already satisfied: setuptools in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from wandb) (68.0.0)\r\n",
      "Collecting appdirs>=1.4.3 (from wandb)\r\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from wandb) (4.7.1)\r\n",
      "Collecting protobuf!=4.21.0,<5,>=3.19.0 (from wandb)\r\n",
      "  Obtaining dependency information for protobuf!=4.21.0,<5,>=3.19.0 from https://files.pythonhosted.org/packages/98/d6/6e2f5047b9b66a57654368121dde2dc86b5ea5d7bb887e620587389dab5e/protobuf-4.24.1-cp37-abi3-macosx_10_9_universal2.whl.metadata\r\n",
      "  Downloading protobuf-4.24.1-cp37-abi3-macosx_10_9_universal2.whl.metadata (540 bytes)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\r\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.7/62.7 kB\u001B[0m \u001B[31m2.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\r\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\r\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\r\n",
      "Downloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m20.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading GitPython-3.1.32-py3-none-any.whl (188 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m188.5/188.5 kB\u001B[0m \u001B[31m26.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading protobuf-4.24.1-cp37-abi3-macosx_10_9_universal2.whl (409 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m409.3/409.3 kB\u001B[0m \u001B[31m28.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m215.6/215.6 kB\u001B[0m \u001B[31m27.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hBuilding wheels for collected packages: pathtools\r\n",
      "  Building wheel for pathtools (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=ef0bbd66a93e45498942c9216d2442e4de25f353f771786e3a7b29117b484b43\r\n",
      "  Stored in directory: /Users/filippocavallari/Library/Caches/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\r\n",
      "Successfully built pathtools\r\n",
      "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, protobuf, docker-pycreds, gitdb, GitPython, wandb\r\n",
      "Successfully installed GitPython-3.1.32 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 protobuf-4.24.1 sentry-sdk-1.29.2 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.8\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "!pip install vmas\n",
    "!pip install Pillow\n",
    "!pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "!pip install ipython\n",
    "!pip install autoreload\n",
    "!pip install torch-geometric\n",
    "!pip install wandb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-22T13:11:23.493348Z"
    }
   },
   "id": "61656dcf3dab2c71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing dependencies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "728c4ace8a8e66ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import threading\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import tensor, Tensor\n",
    "from vmas import make_env\n",
    "\n",
    "import wandb\n",
    "from Cleaning import Scenario as CleaningScenario\n",
    "from DeepQLearner import DeepQLearner\n",
    "from LearningConfiguration import LearningConfiguration, NNFactory\n",
    "from ReplayBuffer import ReplayBufferFactory\n",
    "import Device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4831fcc8816f85a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initializing wandb\n",
    "Wandb is the online platform used to track the training process and store the results. It is used to log the reward, the loss, the epsilon value and the mean reward for each episode."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4debdbb3ded0cbf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"0\" #TODO: Add your API key here\n",
    "run = wandb.init(project=\"vmas\", reinit=True, config={\n",
    "    \"learning_rate\": 0.0005,\n",
    "    \"architecture\": \"MLP\",\n",
    "    #\"epochs\": n_steps\n",
    "})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "550f770bcc3ae9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining the scenario\n",
    "The CleaningScenario class defines the number of agents and targets, the size of the environment, the action space, the reward function and other parameters such as the number of epochs and the number of episodes (or steps).\n",
    "The reward function in the \"Cleaning Agents\" scenario is designed to encourage agents to make optimal decisions while cleaning the targets. The reward function operates as follows:\n",
    "\n",
    "1. When an agent's distance from a target falls below a predefined threshold (K), indicating successful cleaning, the agent is rewarded positively: $$Reward = 1 + \\text{number of previously removed targets}$$\n",
    "\n",
    "2. If an agent's LIDAR sensor does not detect any targets in its vicinity, it receives a negative reward (-1): $$Reward = -1$$\n",
    "\n",
    "3. When an agent's LIDAR sensor detects a target, and the distance from the agent's previous position to the target decreases, the agent is rewarded based on the function: $$Reward = \\frac{-\\text{LIDAR range}}{x}$$ (normalized to fall within the range of -0.5 to 0)\n",
    "\n",
    "4. Conversely, if the distance from the agent's previous position to the target increases, indicating a suboptimal move, the agent is penalized based on the same function: $$Reward = \\frac{-\\text{LIDAR range}}{x}$$ (normalized to fall within the range of -1 to -0.5)\n",
    "\n",
    "The strategic use of negative rewards, except for successful target removal, ensures that agents are motivated to make coordinated and efficient moves. It encourages agents to continually improve their performance by minimizing suboptimal actions and maximizing successful cleaning operations.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "861d96796fcc6d4f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scenario_name = CleaningScenario()\n",
    "\n",
    "# Scenario specific variables\n",
    "n_agents = 1\n",
    "n_targets = 8\n",
    "num_envs = 1  # Number of vectorized environments\n",
    "continuous_actions = True\n",
    "device = Device.get()  # or cuda or any other torch device\n",
    "n_steps = 1000  # Number of steps before returning done\n",
    "n_epochs = 1000\n",
    "dict_spaces = True  # Weather to return obs, rewards, and infos as dictionaries with agent names (by default they are lists of len # of agents)\n",
    "\n",
    "dataset_size = 10000\n",
    "\n",
    "frame_list = []  # For creating a gif\n",
    "init_time = time.time()\n",
    "step = 0\n",
    "\n",
    "# Actions\n",
    "speed = 0.5\n",
    "north = tensor([0, -1*speed])\n",
    "south = tensor([0, speed])\n",
    "east = tensor([speed, 0])\n",
    "west = tensor([-1*speed, 0])\n",
    "#stop = tensor([0, 0])\n",
    "ne = tensor([speed, -1*speed])\n",
    "nw = tensor([-1*speed, -1*speed])\n",
    "se = tensor([speed, speed])\n",
    "sw = tensor([-1*speed, speed])\n",
    "\n",
    "lidar_measure_shape = 50# * 2\n",
    "pos_shape = 2\n",
    "vel_shape = 2\n",
    "tot_shape = lidar_measure_shape# + pos_shape + vel_shape\n",
    "\n",
    "actions = [north, south, east, west, ne, nw, se, sw]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffd7cb8985d5fa81"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Deep Q-Learning\n",
    "My implementation of Deep Q Learning (DQL) for Multilayer Perceptron (MLP) networks is designed to accommodate the unique characteristics of the Vectorized Multi-Agent Simulator (VMAS) environment. VMAS employs additional dimensions to parallelize environments and agents, necessitating a flexible approach to handle complex tensor shapes.\n",
    "\n",
    "##### State Representation\n",
    "\n",
    "In this implementation, I employ a state representation that aligns with the structure of VMAS. Specifically, the state of each agent is represented as a tensor of shape `[50, 1]`. This choice is informed by the fact that each agent's LIDAR sensor in VMAS emits 50 rays, returning float distances. The positions of the individual agents within the environment are not considered relevant for decision-making since agents rely solely on LIDAR measurements, and the positions of targets can vary across different VMAS environments.\n",
    "\n",
    "##### Tensor Shapes\n",
    "\n",
    "To facilitate the training and execution of the MLP networks within VMAS, my network architecture accommodates tensors with the shape `[number of environments, number of agents, 50, 1]`. This tensor shape aligns with VMAS's parallelized structure, allowing the network to process data from multiple environments and agents simultaneously.\n",
    "\n",
    "By adapting my DQL implementation to handle tensors of complex shapes, I ensure seamless integration with VMAS's multi-agent framework. This flexibility enables agents to effectively learn and make decisions within the dynamic and parallelized VMAS environment, ultimately enhancing their performance in complex multi-agent tasks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce906183c59eede3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#learning_configuration = LearningConfiguration(update_each=math.floor(n_steps/3),dqn_factory=NNFactory(tot_shape,64,len(actions)))\n",
    "learning_configuration = LearningConfiguration(update_each=200,dqn_factory=NNFactory(tot_shape,64,len(actions)))\n",
    "\n",
    "dql = DeepQLearner(\n",
    "    memory=ReplayBufferFactory(dataset_size),\n",
    "    action_space=actions,\n",
    "    learning_configuration=learning_configuration\n",
    ")\n",
    "\n",
    "#dql.load_snapshot(\"./-38-2023-09-21-23-55-46-agent-0\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d50903fb76aec43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utility functions\n",
    "Some utility functions to save gifs of the training process and to check if an environment is done."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa30d1383733896e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "targets_pos = []\n",
    "\n",
    "for i in range(n_targets):\n",
    "    targets_pos.append(tensor([random.random() * random.randint(-1, 1), random.random() * random.randint(-1, 1)], device=Device.get()))\n",
    "\n",
    "def isOneEnvDone(info_array):\n",
    "    tensor = info_array[\"agent_0\"][\"active_targets\"]\n",
    "    for i in range(num_envs):\n",
    "        if tensor[i] == 0:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def save_gif(frame_list, epoch):\n",
    "    for i in range(1):\n",
    "        gif_name = scenario_name.__class__.__name__ + \"-env-\" + str(i) + \"-epoch-\" + str(epoch) + \".gif\"\n",
    "        frame_list[i].save(\n",
    "            gif_name,\n",
    "            save_all=True,\n",
    "            append_images=frame_list[1:],\n",
    "            duration=1,\n",
    "            loop=0,\n",
    "        )\n",
    "    dql.snapshot(epoch, \"0\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27972fc6d4173515"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training process\n",
    "The core DQL algorithm can be summarized in several steps:\n",
    "\n",
    "1. Initialize Q-network and target network with random weights.\n",
    "2. Initialize replay buffer.\n",
    "3. For each episode:\n",
    "   1. Observe the current state `s`.\n",
    "   2. Select an action `a` using $\\epsilon$-greedy policy.\n",
    "   3. Execute action `a`, observe reward `r` and next state `s'`.\n",
    "   4. Store the experience `(s, a, r, s')` in the replay buffer.\n",
    "   5. Sample a mini-batch from the replay buffer.\n",
    "   6. Calculate the target Q-values using the target network.\n",
    "   7. Calculate the loss between predicted and target Q-values using the Bellman equation.\n",
    "   8. Update the Q-network weights using backpropagation.\n",
    "   9. Periodically update the target network weights.\n",
    "10. Repeat until convergence or a predetermined number of episodes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1725ad909d80634"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for e in range(0, n_epochs):\n",
    "    env = make_env(\n",
    "        scenario=scenario_name,\n",
    "        num_envs=num_envs,\n",
    "        device=device,\n",
    "        continuous_actions=continuous_actions,\n",
    "        dict_spaces=dict_spaces,\n",
    "        wrapper=None,\n",
    "        seed=None,\n",
    "        n_targets=n_targets,\n",
    "        n_agents=n_agents,\n",
    "        wandb=wandb,\n",
    "        targets_pos=targets_pos\n",
    "    )\n",
    "    previous_states = {}\n",
    "    for step in range(1, n_steps):\n",
    "        print(f\"Step {step}\")\n",
    "        actions = {}\n",
    "        logs = {}\n",
    "        for i, agent in enumerate(env.agents):\n",
    "            lidar_measure = previous_states[agent.name][\"lidar_measure\"] if step > 1 else torch.zeros(num_envs, lidar_measure_shape).to(Device.get())\n",
    "            positions = agent.state.pos\n",
    "            velocities = agent.state.vel\n",
    "            agent_actions_list = []\n",
    "            for j in range(num_envs):\n",
    "                state = lidar_measure[j]#torch.cat((positions[j], velocities[j], lidar_measure[j]),dim=-1).to(Device.get())\n",
    "                action = dql.behavioural(state)\n",
    "                #print(action)\n",
    "                agent_actions_list.append(action)\n",
    "            agent_actions = torch.stack(agent_actions_list)\n",
    "            actions.update({agent.name: agent_actions})\n",
    "            if step > dql.batch_size/num_envs:\n",
    "                dql.improve() # Improve the model\n",
    "                #TODO Should I do the improve once for each env or once for each agent?\n",
    "        obs, rewards, dones, info = env.step(actions)\n",
    "        mean_reward = 0\n",
    "        #print(rewards)\n",
    "        for i, agent in enumerate(env.agents):\n",
    "            positions = agent.state.pos\n",
    "            velocities = agent.state.vel\n",
    "            lidar_measure = obs[agent.name][:, (tot_shape - lidar_measure_shape):]\n",
    "            previous_states.update({agent.name: {\"lidar_measure\": lidar_measure, \"pos\": positions, \"vel\": velocities}})\n",
    "            for j in range(num_envs):\n",
    "                reward = rewards[agent.name][j]\n",
    "                mean_reward += reward\n",
    "                logs.update({f\"reward_{agent.name}_env_{j}\": reward})\n",
    "                prev_state = previous_states[agent.name]\n",
    "                prev_state = prev_state[\"lidar_measure\"][j]#torch.cat((prev_state[\"pos\"][j], prev_state[\"vel\"][j], prev_state[\"lidar_measure\"][j]),dim=-1).to(Device.get())\n",
    "                state = obs[agent.name][j]\n",
    "                action = actions[agent.name][j]\n",
    "                dql.record(prev_state,action,reward,state)\n",
    "        mean_reward /= (num_envs*n_agents)\n",
    "        logs.update({\"epsilon\": dql.epsilon.value()})\n",
    "        logs.update({\"loss\": dql.last_loss})\n",
    "        logs.update({\"mean_reward\": mean_reward})\n",
    "        logs.update({f\"mean_reward_epoch_{e}\": mean_reward})\n",
    "    \n",
    "        wandb.log(logs)\n",
    "        dql.epsilon.update() # Update epsilon\n",
    "        #dql.snapshot(step, \"0\")\n",
    "        frame_list.append(\n",
    "            Image.fromarray(env.render(mode=\"rgb_array\", agent_index_focus=None))\n",
    "        )  # Can give the camera an agent index to focus on\n",
    "        \n",
    "        #print(info)\n",
    "        if isOneEnvDone(info):\n",
    "            print(\"Env done\")\n",
    "            dql.target_network.load_state_dict(dql.policy_network.state_dict())\n",
    "            break\n",
    "    \n",
    "    \n",
    "    # Produce a gif\n",
    "    frame_list_copy = copy.deepcopy(frame_list)\n",
    "    thread = threading.Thread(target=save_gif, args=(frame_list_copy, e))\n",
    "    \n",
    "    thread.start()\n",
    "    \n",
    "    frame_list.clear()\n",
    "    \n",
    "    total_time = time.time() - init_time\n",
    "    print(\n",
    "        f\"It took: {total_time}s for {n_steps} steps of {num_envs} parallel environments on device {device} \"\n",
    "        f\"for {scenario_name} scenario.\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e36043d3bc405e42"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Performance Results: MLP Networks in the Cleaning Agents Scenario\n",
    "\n",
    "The performance evaluation of my Multilayer Perceptron (MLP) network-based agents in the Cleaning Agents scenario provides valuable insights into their learning and decision-making capabilities. The training process consisted of 150 epochs, each comprising 1000 steps. If an agent successfully achieved its task before completing all the steps in an epoch, it proceeded to the next epoch. Here, I present a comprehensive analysis of the training and evaluation results.\n",
    "\n",
    "#### Training Progress\n",
    "\n",
    "![Reward and Loss comparison](https://i.imgur.com/hwZ7rNF.png \"Reward and Loss comparison\")\n",
    "\n",
    "During the initial epoch, agents exhibited limited familiarity with the task of removing targets from the 2D environment. Consequently, the mean reward achieved by the agents in this phase remained relatively low, with a maximum reward of approximately 3.0. As training progressed, agents displayed improved performance. By the 70th epoch, the highest recorded mean reward reached 8.0, typically occurring at around 900 steps into the episode. This milestone indicated that agents had grasped the fundamentals of their task but still had room for refinement.\n",
    "\n",
    "A significant leap in performance was observed at the 145th epoch. Agents demonstrated remarkable efficiency by removing almost all targets in fewer than 500 steps, with the last remaining target eliminated in the final steps of the episode. Concurrently, the loss function exhibited a notable pattern. While initially showing spikes when agents were rewarded values exceeding 1.0, the loss gradually minimized and stabilized near zero after the 100th epoch.\n",
    "\n",
    "![Mean rewards](https://i.imgur.com/QVtu3HF.png \"mean rewards\")\n",
    "\n",
    "#### Evaluation Results\n",
    "\n",
    "![Different stages of a simulation with one agent and 8 targets](https://i.imgur.com/unYsDTi.png \"Different stages of a simulation with one agent and 8 targets\")\n",
    "\n",
    "To assess the agents' performance in practical scenarios, I conducted evaluations with varying numbers of agents and targets. In a scenario involving one agent and eight targets, the results indicated the following:\n",
    "* On average, it took approximately 10 steps for the agent to remove the first target, reflecting a quick initiation of the cleaning process.\n",
    "* Removal of half the targets occurred, on average, in approximately 170 steps, demonstrating consistent progress.\n",
    "* To remove all eight targets, agents required an average of around 1440 steps, highlighting the complexity of clearing the entire environment.\n",
    "\n",
    "![Different stages of a simulation with four agent and 14 targets](https://i.imgur.com/acA7fVT.png \"Different stages of a simulation with four agent and 14 targets\")\n",
    "\n",
    "In a more challenging scenario with four agents and fourteen targets, the outcomes were as follows:\n",
    "* Agents demonstrated improved collaboration, taking an average of about 400 steps to clear all fourteen targets.\n",
    "* The first target was removed in an average of 10 steps, emphasizing swift task initiation.\n",
    "* Removal of half the targets was achieved in approximately 240 steps, showcasing efficient teamwork among agents.\n",
    "\n",
    "![Remaining targets at each step](https://i.imgur.com/8eLGGts.png \"Remaining targets at each step\")\n",
    "\n",
    "These performance results underscore the adaptability and learning capabilities of my MLP network-based agents in the Cleaning Agents scenario. As training progressed, agents demonstrated a substantial improvement in their task execution, achieving efficient target removal even in scenarios with multiple agents and numerous targets. These findings reflect the effectiveness of my reinforcement learning approach in addressing complex multi-agent coordination tasks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "546cc4e5b9e6a395"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "648e035c7a994f7d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
