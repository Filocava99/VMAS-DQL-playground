{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vmas in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (1.2.11)\r\n",
      "Requirement already satisfied: numpy in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from vmas) (1.24.4)\r\n",
      "Requirement already satisfied: torch in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from vmas) (2.1.0.dev20230809)\r\n",
      "Requirement already satisfied: pyglet<=1.5.27 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from vmas) (1.5.27)\r\n",
      "Requirement already satisfied: gym in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from vmas) (0.26.2)\r\n",
      "Requirement already satisfied: six in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from vmas) (1.16.0)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from gym->vmas) (2.2.1)\r\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from gym->vmas) (0.0.8)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from gym->vmas) (6.0.0)\r\n",
      "Requirement already satisfied: filelock in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch->vmas) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch->vmas) (4.7.1)\r\n",
      "Requirement already satisfied: sympy in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch->vmas) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch->vmas) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch->vmas) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch->vmas) (2023.4.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym->vmas) (3.11.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from jinja2->torch->vmas) (2.1.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from sympy->torch->vmas) (1.3.0)\r\n",
      "Requirement already satisfied: Pillow in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (10.0.0)\r\n",
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cpu\r\n",
      "Requirement already satisfied: torch in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (2.1.0.dev20230809)\r\n",
      "Requirement already satisfied: torchvision in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (0.16.0.dev20230809)\r\n",
      "Requirement already satisfied: torchaudio in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (2.1.0.dev20230809)\r\n",
      "Requirement already satisfied: filelock in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch) (4.7.1)\r\n",
      "Requirement already satisfied: sympy in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch) (2023.4.0)\r\n",
      "Requirement already satisfied: numpy in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torchvision) (1.24.4)\r\n",
      "Requirement already satisfied: requests in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torchvision) (2.31.0)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torchvision) (10.0.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from jinja2->torch) (2.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests->torchvision) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests->torchvision) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests->torchvision) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests->torchvision) (2023.7.22)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: ipython in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (8.12.2)\r\n",
      "Requirement already satisfied: backcall in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (0.2.0)\r\n",
      "Requirement already satisfied: decorator in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (5.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (0.18.1)\r\n",
      "Requirement already satisfied: matplotlib-inline in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (0.1.6)\r\n",
      "Requirement already satisfied: pickleshare in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (0.7.5)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (3.0.36)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (2.15.1)\r\n",
      "Requirement already satisfied: stack-data in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (0.2.0)\r\n",
      "Requirement already satisfied: traitlets>=5 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (5.7.1)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (4.7.1)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (4.8.0)\r\n",
      "Requirement already satisfied: appnope in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from ipython) (0.1.2)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from jedi>=0.16->ipython) (0.8.3)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from pexpect>4.3->ipython) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython) (0.2.5)\r\n",
      "Requirement already satisfied: executing in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from stack-data->ipython) (0.8.3)\r\n",
      "Requirement already satisfied: asttokens in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from stack-data->ipython) (2.0.5)\r\n",
      "Requirement already satisfied: pure-eval in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from stack-data->ipython) (0.2.2)\r\n",
      "Requirement already satisfied: six in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from asttokens->stack-data->ipython) (1.16.0)\r\n",
      "Requirement already satisfied: autoreload in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (0.1.2)\r\n",
      "Requirement already satisfied: selenium in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from autoreload) (4.11.2)\r\n",
      "Requirement already satisfied: click in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from autoreload) (8.1.6)\r\n",
      "Requirement already satisfied: watchdog in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from autoreload) (3.0.0)\r\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from selenium->autoreload) (1.26.16)\r\n",
      "Requirement already satisfied: trio~=0.17 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from selenium->autoreload) (0.22.2)\r\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from selenium->autoreload) (0.10.3)\r\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from selenium->autoreload) (2023.7.22)\r\n",
      "Requirement already satisfied: attrs>=20.1.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from trio~=0.17->selenium->autoreload) (22.1.0)\r\n",
      "Requirement already satisfied: sortedcontainers in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from trio~=0.17->selenium->autoreload) (2.4.0)\r\n",
      "Requirement already satisfied: idna in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from trio~=0.17->selenium->autoreload) (3.4)\r\n",
      "Requirement already satisfied: outcome in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from trio~=0.17->selenium->autoreload) (1.2.0)\r\n",
      "Requirement already satisfied: sniffio in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from trio~=0.17->selenium->autoreload) (1.2.0)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from trio~=0.17->selenium->autoreload) (1.1.3)\r\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from trio-websocket~=0.9->selenium->autoreload) (1.2.0)\r\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from urllib3[socks]<3,>=1.26->selenium->autoreload) (1.7.1)\r\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->autoreload) (0.14.0)\r\n",
      "Requirement already satisfied: torch-geometric in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (2.3.1)\r\n",
      "Requirement already satisfied: tqdm in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch-geometric) (4.66.1)\r\n",
      "Requirement already satisfied: numpy in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch-geometric) (1.24.4)\r\n",
      "Requirement already satisfied: scipy in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch-geometric) (1.10.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch-geometric) (3.1.2)\r\n",
      "Requirement already satisfied: requests in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch-geometric) (2.31.0)\r\n",
      "Requirement already satisfied: pyparsing in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch-geometric) (3.1.1)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch-geometric) (1.3.0)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from torch-geometric) (5.9.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from jinja2->torch-geometric) (2.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests->torch-geometric) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests->torch-geometric) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests->torch-geometric) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests->torch-geometric) (2023.7.22)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from scikit-learn->torch-geometric) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from scikit-learn->torch-geometric) (3.2.0)\r\n",
      "Collecting wandb\r\n",
      "  Obtaining dependency information for wandb from https://files.pythonhosted.org/packages/ed/d7/8927aef63869d5d379adb63dc97f9cbc53830fdf85457b84a156fabcb231/wandb-0.15.8-py3-none-any.whl.metadata\r\n",
      "  Downloading wandb-0.15.8-py3-none-any.whl.metadata (8.3 kB)\r\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from wandb) (8.1.6)\r\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\r\n",
      "  Obtaining dependency information for GitPython!=3.1.29,>=1.0.0 from https://files.pythonhosted.org/packages/67/50/742c2fb60989b76ccf7302c7b1d9e26505d7054c24f08cc7ec187faaaea7/GitPython-3.1.32-py3-none-any.whl.metadata\r\n",
      "  Downloading GitPython-3.1.32-py3-none-any.whl.metadata (10.0 kB)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from wandb) (2.31.0)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from wandb) (5.9.0)\r\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\r\n",
      "  Obtaining dependency information for sentry-sdk>=1.0.0 from https://files.pythonhosted.org/packages/86/bb/ecb87fd214d5bbade07edf2ecdd829cf346e5b552689d6228112c6517286/sentry_sdk-1.29.2-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl.metadata (8.8 kB)\r\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\r\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\r\n",
      "Requirement already satisfied: PyYAML in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from wandb) (6.0)\r\n",
      "Collecting pathtools (from wandb)\r\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting setproctitle (from wandb)\r\n",
      "  Downloading setproctitle-1.3.2-cp38-cp38-macosx_10_9_universal2.whl (16 kB)\r\n",
      "Requirement already satisfied: setuptools in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from wandb) (68.0.0)\r\n",
      "Collecting appdirs>=1.4.3 (from wandb)\r\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from wandb) (4.7.1)\r\n",
      "Collecting protobuf!=4.21.0,<5,>=3.19.0 (from wandb)\r\n",
      "  Obtaining dependency information for protobuf!=4.21.0,<5,>=3.19.0 from https://files.pythonhosted.org/packages/98/d6/6e2f5047b9b66a57654368121dde2dc86b5ea5d7bb887e620587389dab5e/protobuf-4.24.1-cp37-abi3-macosx_10_9_universal2.whl.metadata\r\n",
      "  Downloading protobuf-4.24.1-cp37-abi3-macosx_10_9_universal2.whl.metadata (540 bytes)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\r\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.7/62.7 kB\u001B[0m \u001B[31m2.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/filippocavallari/anaconda3/envs/deeplearning/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\r\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\r\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\r\n",
      "Downloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m20.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading GitPython-3.1.32-py3-none-any.whl (188 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m188.5/188.5 kB\u001B[0m \u001B[31m26.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading protobuf-4.24.1-cp37-abi3-macosx_10_9_universal2.whl (409 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m409.3/409.3 kB\u001B[0m \u001B[31m28.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m215.6/215.6 kB\u001B[0m \u001B[31m27.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hBuilding wheels for collected packages: pathtools\r\n",
      "  Building wheel for pathtools (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=ef0bbd66a93e45498942c9216d2442e4de25f353f771786e3a7b29117b484b43\r\n",
      "  Stored in directory: /Users/filippocavallari/Library/Caches/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\r\n",
      "Successfully built pathtools\r\n",
      "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, protobuf, docker-pycreds, gitdb, GitPython, wandb\r\n",
      "Successfully installed GitPython-3.1.32 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 protobuf-4.24.1 sentry-sdk-1.29.2 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.8\r\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import Device\n",
    "!pip install vmas\n",
    "!pip install Pillow\n",
    "!pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "!pip install ipython\n",
    "!pip install autoreload\n",
    "!pip install torch-geometric\n",
    "!pip install wandb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-22T13:11:30.853914Z",
     "start_time": "2023-08-22T13:11:23.493348Z"
    }
   },
   "id": "61656dcf3dab2c71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:524c8emc) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>▃▂▂▂▄▄▁▄▃▂▃▃▃▄▂▁▁▆▄▁▁▄▂▁▁▁▂▁▁▁▄▃▁▁▂▁█▁▂▁</td></tr><tr><td>mean_reward</td><td>▃▃▃▄▃▃▄▃▃▃▂▂▂▂▄▂▁▄▃▂▃▃▂▂▃▂▃▃▂▃▂▂▂▁▄▃▄▃▄█</td></tr><tr><td>mean_reward_epoch_0</td><td>▆▇▇▇▆▆▄▄▄▅▄▄▆▅▄▄▄▄▄▄▄▆▆▄▅▇▇▅▄▄▅▇▁▄▆███▆▄</td></tr><tr><td>mean_reward_epoch_1</td><td>▃▃▃▃▃▃▄▃▂▂▂▂▂▂▂▃▃▄▄▄▃▂▂▁▁▁▂▂▂▂▂▁▁▂▃▃▄▃██</td></tr><tr><td>mean_reward_epoch_10</td><td>▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃█████████▁▁▂▂▂▂▂▂▃▂▁▁▁▂</td></tr><tr><td>mean_reward_epoch_11</td><td>▃▃▂▁▁▂▁▂▁▁▁▂▁▂▁▁▁▂▁▁▁▂▂▂▁▁▁▂▃▃▃▃▃▃▃▃▃█▃▃</td></tr><tr><td>mean_reward_epoch_12</td><td>▁▁▁█▂█▁▂▂▂▂▂▂▂▂▂▂▂▂█████████████████▁█▂▂</td></tr><tr><td>mean_reward_epoch_13</td><td>▃▁▁▂▂▃▃▃▃▃▂▂▂▃▂▁▂▃▂▂▂▂▂▃▂▃▂▃█▃▂▂▂▃▃▃▃▃▃▂</td></tr><tr><td>mean_reward_epoch_14</td><td>▂▁▁▁▁▁▁▁▁▁▂▃██▃██▂▂▂▂▂▃▃▃▃█▃▂▂▂▂▂▂▁▁▁▁▁▂</td></tr><tr><td>mean_reward_epoch_15</td><td>▃▁▂▃█▃▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃██▂▃▂▁▁▁▂▃</td></tr><tr><td>mean_reward_epoch_16</td><td>▂▃▂▃▂▂▁▁▂▁▁▂▁▁▁▂▂▂▂▂▂▂▂▂▃▃▂▂▂▂▂▂▂█▂▁▁▂▂▂</td></tr><tr><td>mean_reward_epoch_17</td><td>▄▃▃▃▃▃▃▃█▃▃▄▄▄▄▄▃▄▃▄▄▂▃▃▂▁▁▁▂▂▃▂▂▂▂▂▂▂▂▂</td></tr><tr><td>mean_reward_epoch_18</td><td>▂▂▂▂▂▂▃▂▂▂▂▂▃▂▂▂▂▂▂▃█▃▃▂█▁▁▁▂▂▂▂▂▂█▂▂▂▁▁</td></tr><tr><td>mean_reward_epoch_19</td><td>▃▃▃▃▂▂▂▂▂▂▂▂▃▃█▃▃▃▃█▃▂▃▃▂▂▂▂▂▃▁▁▁▂▂▂▃▃▂▃</td></tr><tr><td>mean_reward_epoch_2</td><td>▁▁▂█████████████████████████████████████</td></tr><tr><td>mean_reward_epoch_20</td><td>█▂▃▃█▃▃▃▄▃▃▃▄█▄▃▃▃▂▃▃▂▂▃▂▁▂▃▂▂▃▃▂▃▃▃▃▃▃▃</td></tr><tr><td>mean_reward_epoch_21</td><td>▃▄▄▄█████▄▄▃▃▃▃▄▃▃▃▃▄▃▃▃▃▃▃▃▃▃█▁▁▁▁▁▁▁▁▂</td></tr><tr><td>mean_reward_epoch_22</td><td>█▂▁▁▁▁▂▃▄▄▄█▃▃▃▃███▃▄█▄▂▂▂▂▂▃▄▃▃▃▄▃▃▃▃▃▃</td></tr><tr><td>mean_reward_epoch_23</td><td>▃▃▃▃▃▃█▁▁▁▁▁▂▂▂▁▂▂▂▃█▃▂█▃▂▃█▃▂▂▁▁▁▂▁▁▁▁▁</td></tr><tr><td>mean_reward_epoch_24</td><td>█▂▂▂▁▂▃▂▂▂▂▂▂▃▃▃█▃▃▄▃▄▃▄▄█▂▃▃▃▃▃▃▃▃▃▃▂▂▂</td></tr><tr><td>mean_reward_epoch_25</td><td>▆▄▁▄▄▆█▆▆▆▆▃▃▂▄▅▃▄▄▃▃▃▄▅▆▅▆▅▄▃▅▄▅▅▄▅▆▆▄▄</td></tr><tr><td>mean_reward_epoch_26</td><td>▇██▇▅▅▄▅▅▃▅▆▅▆▆▆▅▆▆▄▂▂▂▃▁▁▃▅▃▅▅▅▆▆▄▆▄▆▆▃</td></tr><tr><td>mean_reward_epoch_27</td><td>█▁▂▂▂▁▂▃▂▂▂▂▂▁▂▂▁▂▂▁▂▂▂▃▂▂▂▂▁▂▂▂▂▃▃▂▂▂▂▂</td></tr><tr><td>mean_reward_epoch_28</td><td>██▇▇▆▄▄▅▆▆▅▅▄▅▃▃▄▂▂▄▃▂▃▃▃▃▃▂▃▁▃▅▃▂▄▃▂▄▃▂</td></tr><tr><td>mean_reward_epoch_29</td><td>▂▂▃▃▂█▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂██▂▁▂▁▁▂▂▂▂▁▁▁▂</td></tr><tr><td>mean_reward_epoch_3</td><td>▆▅▆▆▄▅▅▅▅▇████████████▇▇▇▇▇▇▆▆▇▂▁▁▃▅▇▆▂▂</td></tr><tr><td>mean_reward_epoch_30</td><td>▃██████▄▃▃▃▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃█▁▂▂▁▂▂▂▂▁▁▁▁▁</td></tr><tr><td>mean_reward_epoch_31</td><td>█▇▆▇█▂▁▁▂▄▄▂▁▁▁▁▁▁▁▁▂▁▂▃▂▂▃▃▃▄▁▄▄▅▅▆▇▅▅▆</td></tr><tr><td>mean_reward_epoch_32</td><td>█▆▅▆▆▅▆▆▆▇▅▆▃▃▂▃▂▂▄▄▃▄▅▅▅▃▄▅▃▁▂▂▂▂▂▅▄▅▇▄</td></tr><tr><td>mean_reward_epoch_33</td><td>▃▃▂▃▃▃▃█▁▁▁▁▁▂▂▃▂▂▃▃▃▃▂▂▂▃▃▂▃▂▂▂▂▂▃▃▃▂▁▁</td></tr><tr><td>mean_reward_epoch_34</td><td>█▇▇▇█▆▆█▇▇▆▇▇▆▇▆▄▂▂▃▁▄▄▅▄▅▄▁▂▁▂▂▂▂▂▂▂▂▂▄</td></tr><tr><td>mean_reward_epoch_35</td><td>▅▂▂▂▃▁▃▄▃▂▂▂▃▃▄▅▃▃▄▅▅▅▆▄▅▇▇▆▇▇██▇█▇▇▇▇▇▇</td></tr><tr><td>mean_reward_epoch_36</td><td>█▂▂▃▃▂▃▃▃▂▂▁▂▂▂▂▂▂▂▂▃▂▂▂▁▁▁▁▂▂▁▂▁▁▂▁▁▂▁▁</td></tr><tr><td>mean_reward_epoch_37</td><td>█▄▅▅▆▅▇▅▅▆▄▂▂▂▂▁▁▁▁▁▂▁▂▄▄▄▅▄▆▅▅▅▅▄▄▄▅▅▅▅</td></tr><tr><td>mean_reward_epoch_38</td><td>█▇█▄▅▆▅▅▅▅▄▃▅▆▄▅▄▃▃▂▁▁▁▂▁▁▁▂▁▂▁▂▄▅▅▇▆▅▄▅</td></tr><tr><td>mean_reward_epoch_39</td><td>▇▆▆▆▇█▆▆▆▆▆▆▇█▇█▄▄▄▄▄▄▅▄▄▄▅▄▄▅▄▄▂▂▁▂▂▁▁▁</td></tr><tr><td>mean_reward_epoch_4</td><td>▃▄▃▄▃▄▃▃▂▂▃▃▄▃▃█▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▂▂▂▂</td></tr><tr><td>mean_reward_epoch_40</td><td>▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄█▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁</td></tr><tr><td>mean_reward_epoch_41</td><td>▆▄▆▆▆▄▅▄▅▃▃▄▆█▆▃▄▁▁▁▁▁▁▂▃▂▃▂▂▂▁▃▄▄▄▄▄▅▆▅</td></tr><tr><td>mean_reward_epoch_42</td><td>▄▃▅▅▆▃▂▅▄▄▃▃▃▃▂▄▃▁▅▃▅▆▆▄▅▇██▆▆▅▄▅▅▅▇▄▃▅▃</td></tr><tr><td>mean_reward_epoch_43</td><td>█▃▄▄▄▄▃▄▁▂▃▃▂▃▄▆▅▅▄▃▃▃▃▃▃▄▃▄▃▃▃▄▆▅▃▃▃▃▃▄</td></tr><tr><td>mean_reward_epoch_44</td><td>█▇█▃▄▃▃▃▃▃▄▆▅▅▆▅▄▃▂▂▃▄▃▁▁▁▃▂▂▃▄▄▄▃▃▄▅▅▄▄</td></tr><tr><td>mean_reward_epoch_45</td><td>▅▄▇▆▆▄▆▄▃▅▅▂▃▂▄▂▄▅▃▃▃▃▄▅▃▃▅█▅▆▇▅▇▄▆▂▂▂▁▁</td></tr><tr><td>mean_reward_epoch_46</td><td>█▆▅▅▄▃▁▂▂▃▅▂▁▂▃▅▇▅▅▅▅▂▂▂▄▄▆▃▄▇▅▆▃▅▆▇▇▆▄▄</td></tr><tr><td>mean_reward_epoch_47</td><td>▅▃▃▅▅▇▆▇▇▇█▇▃▂▁▂▁▃▅▆▅▅▆▅▄▂▂▃▂▄▄▅▆▅▆▅▇▅▄▅</td></tr><tr><td>mean_reward_epoch_48</td><td>▆▂▂▂▃▂▅▄▄▂▁▁▅▃▂▄▆▆█▇▆▅▄▅█▇▅█▇▇▃▄▆▅▆▇▇▃▅▅</td></tr><tr><td>mean_reward_epoch_49</td><td>▃██████▃▂▂█▂▂▃▃▃▃▃▃▃▃▃▃▃▃█▂▂▂▂▂▁▁▁▁▁▂▂▃▃</td></tr><tr><td>mean_reward_epoch_5</td><td>▃▃▄▄█▃▃▃▃▂▂▃▄▄▄▄▄█▃▃▃▂▂▂▁▂▁▃▃▃▂▂▃▃▃▃▃▃▂▂</td></tr><tr><td>mean_reward_epoch_50</td><td>▃▄▃█▃█▁▂▂▃▃▄▃▃▃▃▃▃▃▃▃▃▃▄▃▃▃▃▃▂▃▂▂▃▃▃▃▃▃▃</td></tr><tr><td>mean_reward_epoch_51</td><td>▃▃▃▁▁▁▁▂▂▂▂▂▁▁▁▁▁▁▁▂▂▂▂▁▂▂▁▂▂▂▂▂▂▂█▃▂▃▃▂</td></tr><tr><td>mean_reward_epoch_52</td><td>▅▁▃▂▄▃▇▆▄▇▆▅▅▃▆▄▅▅▅▄▅▄▃▆▄▂▅▅▆▃▂▄▃▆▅▃▅▇█▇</td></tr><tr><td>mean_reward_epoch_53</td><td>▄▁▁▁▂▃▅▃▃▃▄▂▂▄▂▃▅▄▄▂▃▂▅▇▇█▄▄▆▄▄▄▃▄▄▅▇▇█▇</td></tr><tr><td>mean_reward_epoch_54</td><td>▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃█▄▃▃▃▂▂▃▃▃▃▄▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>mean_reward_epoch_55</td><td>█▆▄▃▁▂▁▂▃▃▄▃▃▂▅▄▄▅▃▃▄▃▄▂▃▃▃▄▅▆▅▅▃▅▃▄▃▄▃▃</td></tr><tr><td>mean_reward_epoch_56</td><td>█▃▇▅▇▇▂▂▅▆▇▅▅▃▅▄▆▂▃▄▁▂▅▄▄▂▄▇▄▇▃▃▃▅▁▃▅▁▁▄</td></tr><tr><td>mean_reward_epoch_57</td><td>█▂▂▂▂▃▃▄▄██▄▃▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃</td></tr><tr><td>mean_reward_epoch_58</td><td>█▇▄▃▄▃▂▅▅▅▆▄▄▆▄▄▅▄▇█▆▅▅▄▆▅▇▆▅▅▅▄▃▁▄▄▆▄▅▆</td></tr><tr><td>mean_reward_epoch_59</td><td>▇▄▂▁▂▅▅▅▆▃▄▅▃▅▃▁▁▂▁▄▆▅▂▆█▅▄▅▃▃▁▅▆█▆▄▅▅▄▅</td></tr><tr><td>mean_reward_epoch_6</td><td>▇██▇█▇▇▇██▇▅▁▄▄▄▅█▆▄▄▄▄▄▄▃▃▃▃▄▄▄▃▄▃▁▁▂▂▂</td></tr><tr><td>mean_reward_epoch_60</td><td>█▆▆▆▅▅▃▂▁▄▆▅▅▃▃▁▄▂▄▄▄▅▄▇▅▅▅▃▄▆▅▆▆▅▅▄▃▄▄▄</td></tr><tr><td>mean_reward_epoch_61</td><td>▂▁▁▁▂▂▂▃▃▂▂▂▂▁▁▁▂▂▂▁▂▂▂▂▂▁▁▁▂▂▂█▃▃▃▃█▃▃▃</td></tr><tr><td>mean_reward_epoch_62</td><td>▃▃▃▃▃▃█▂▁▂▁▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▂▁▁▂▂▁▂▂▂▂▂▂▂</td></tr><tr><td>mean_reward_epoch_63</td><td>▃▃▃▃█▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▂▂▃█▂▂▂▁▂▁▁▂▁▂▁▁▁</td></tr><tr><td>mean_reward_epoch_64</td><td>▅▃▅▆▅▅▆▄▃▃▃▁▃▃▄▅▅▅▃▄▅▄▇█▆▆▆▅▅▅▆▆▆▆▇▆▇▆▅▇</td></tr><tr><td>mean_reward_epoch_65</td><td>█▁▁▁▁▂▂▂▃▂▂▃▃▃▂▂▂▂▂▂▂▂▂▃▃▃▂▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>mean_reward_epoch_66</td><td>█▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>mean_reward_epoch_67</td><td>▃▂▂▂▂▃▃█▄▄▄▄█▃▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▂▂▂▂▂▁▁▁</td></tr><tr><td>mean_reward_epoch_68</td><td>▃▁▃▄▂▃▃▁▃▃▂▃▄▆█▅▇█▇▇█▇▇▇▆▅▃▃▁▂▂▃▄▃▂▅▃▄▅▃</td></tr><tr><td>mean_reward_epoch_69</td><td>▆▇▇█▅▄▅▂▁▁▃▅▅▆█▆▆▆▅▇▄▅▇▄▅▅▃▄█▇▆▇▆▇▄▆▇▅▅▄</td></tr><tr><td>mean_reward_epoch_7</td><td>▃█▂▃▃▃▃▃▃▃▂▃▃▃▃▃▃▂▂▂▃▃▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▂▂▁</td></tr><tr><td>mean_reward_epoch_70</td><td>████████████████▅▅▇▆▆▅▅▇▄▃▃▄▆▇▅▃▅▄▃▂▂▁▁▁</td></tr><tr><td>mean_reward_epoch_71</td><td>▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▂▃▃▂█▂▁▂▂▂▂▂▂▃▂▂▂▂▂</td></tr><tr><td>mean_reward_epoch_72</td><td>▇▃▁▁▂▅██████████▆███▆▅▄▃▁▁▄▂▃▇▇▅▄▂▃▄▇▅▆▅</td></tr><tr><td>mean_reward_epoch_73</td><td>▃▁▁▂▂▁▂▁▁▂▁▁▁▂▂▂▂▂▂▂▂▁▁▂▂▂▃▃▂▂▂▂▂▃▃▃█▃██</td></tr><tr><td>mean_reward_epoch_74</td><td>▃▂█▃▂▁▂▁▁▁▁▁▁▂▁▁▁▁▂▁▁▂▁▂▁▂▂▂▂▂▂▃▂█▂▁▂▁▁▂</td></tr><tr><td>mean_reward_epoch_75</td><td>▂▁▂▂▂▂▁▁▂▁▁▁▂▂▂▂▂▂▃█▂▂▂▁▁▂▁▁▁▁▂▂▁▂▃▂▃█▃▂</td></tr><tr><td>mean_reward_epoch_76</td><td>▂▂▂█▂▁▁▁▁▁▁▁▁▂▂▁▂▁▁▂▂▁▁▂▁▁▁▁▂▂▁▁▁▂▂▂▂▂▂▂</td></tr><tr><td>mean_reward_epoch_77</td><td>▃▃▃▃▃▂▂▂█▂▂▂▂▁▂▁▁▁▁▂▂▁▂▂▁▂▃▂▃▃▃▃▂▃▃▂▃▃▃▃</td></tr><tr><td>mean_reward_epoch_78</td><td>▄▁▃▃▃▃▃▃▄▄▄▂▃▄▄▇█▅▅▇▆▆▇▅▅▅▇█▇▆▅▄▆▄▆█▅▄▂▁</td></tr><tr><td>mean_reward_epoch_79</td><td>▅▂▂▃▃▂▃▄▃▂▁▁▁▁▂▁▂▅▆▅▅▅▃▄▃▄▄▄▆▆█▇▅▆▆▆█▆▃▂</td></tr><tr><td>mean_reward_epoch_8</td><td>▃█▂▂▂▃▄▄▄█▃▃▂▂▂▂▃▂▂▂▁▁▂▃▃▂▂▃▁▁▁▂▂▁▁▁▁▁▁▁</td></tr><tr><td>mean_reward_epoch_80</td><td>▄▁▃▅▄▃▄▃▂▂▂▂▂▂▂▂▅▃▃▆▆▂▂▃▃▅▄▄▃▂▁▅█▅▆▄▅▆▅▂</td></tr><tr><td>mean_reward_epoch_81</td><td>▂▁▁▁▂▃▃▃▃▂▂▂▂▃▂▂▃▃▃▃█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>mean_reward_epoch_82</td><td>▇▇█▆▄▄▅▇▆▆▆▄▆▄▅▆▄▄▄▅▅▃▂▁▁▃▃▄▅▅▆█▃▄▃▄▄▃▃▄</td></tr><tr><td>mean_reward_epoch_83</td><td>█▂▁▁▁▁▁▁▁▂▂▂▂▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_reward_epoch_84</td><td>▂▂█▁▁▁▂▁▁▁▂▁▁▁▁▁▂▁▁▁▂▂▂▂▁▁▁▂▂▂▁▁▁▁▁▁▂▂▃▃</td></tr><tr><td>mean_reward_epoch_85</td><td>█▂▁▂▁▁▁▁▁▂▂▁▂▂▁▂▂▁▂▁▁▁▁▁▁▂▃▃▃▄▃▃▃▃▃▃▃▄██</td></tr><tr><td>mean_reward_epoch_86</td><td>▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▁▁</td></tr><tr><td>mean_reward_epoch_87</td><td>▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂█▂▁▁▁▁▁▁▂▂▁▁▁▁▂▂▁</td></tr><tr><td>mean_reward_epoch_88</td><td>█▂▂▃▃▂▂▂▂▆▂▄▅▂▃▂▃▁▆▂▃▇▅█▅▂▁▃▁▆▁▂▃▃▂▂▃▃▂▅</td></tr><tr><td>mean_reward_epoch_89</td><td>▇▅▅▅█▇▇█▅▆▅▅█▆▆▅▆▅▇▅▅▇▇▆▅▅▅▆▆▅▅▅▇█▁▃▄▅▆▇</td></tr><tr><td>mean_reward_epoch_9</td><td>▃▃▃▃▃▃▂▂▃█▁▁▁▁▂▃▂▂▁▁▂▂▃▂▂▂▂▃▂▂▂▂▃▃▃▂▃▂▂▂</td></tr><tr><td>mean_reward_epoch_90</td><td>▇▄▄▄▆▄▅▂▃▅▄▇▃▁▂▄▃▂▄▇▅▇█▅▆▆▅▅▆▃▅▇▇█▇▆▄▇▇▄</td></tr><tr><td>mean_reward_epoch_91</td><td>▂▂▃▂▂▃▂▃▂▂▁▁▁▁▂▂▂▃▂▂█▃▂▂▂▂▁▁▂▂▁▁▁▁▂▂▂▂▂▂</td></tr><tr><td>mean_reward_epoch_92</td><td>█▅▄▄▅▆▅▆▅▅▅▅▄▃▄▄▆▇▇█▇█▆▇▇▇▅▃▁▂▂▂▂▂▃▂▂▄▇▆</td></tr><tr><td>mean_reward_epoch_93</td><td>▆▄▂▂▁▂▂▄▄▄▆▆▆▆▆█▇▆▆▆▆▆▅▃▄▄▃▄▄▆▅▄▅▄▃▅▆▅▆▆</td></tr><tr><td>mean_reward_epoch_94</td><td>█▃▃▃▂▃▃▄▇▅▅▅▅▆▆▆▅▅▃▄▄▁▃▂▁▁▃▄▃▅▄▄▆▅▅▆▆█▇▆</td></tr><tr><td>mean_reward_epoch_95</td><td>▃▃▃▃▃▃▃▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▃▃▃▂▃█▃▂▂▂▂</td></tr><tr><td>mean_reward_epoch_96</td><td>▇▅█▁▃▁▁▄▃▇▇▇▆▆▇▇▆▆▆█▆▇█▆▇▆▆█▆▆▆▆▆▇▆▆▆▆▆▆</td></tr><tr><td>mean_reward_epoch_97</td><td>▇▄▇▇▄▅▃▂▃▄▅▂▂▁▂▁▂▄▂▁▂▃▄▄▅▅▆█▅▇█▅▆▆▅▆▅▆█▆</td></tr><tr><td>mean_reward_epoch_98</td><td>▂▁▁▁▂▁▁▂█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▂▂▂▁▁▁▂</td></tr><tr><td>mean_reward_epoch_99</td><td>▂▂▂▃▃▃▃▂▃█▃█▃█▂▂▂▂▂▂▂▁▁▁▁▂▂▂▃▃▃▃▂▂▂▂▃▂▂▂</td></tr><tr><td>reward_agent_0_env_0</td><td>▃▃▃▄▃▃▄▃▃▃▂▂▂▂▄▂▁▄▃▂▃▃▂▂▃▂▃▃▂▃▂▂▂▁▄▃▄▃▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>0.01</td></tr><tr><td>loss</td><td>0.00713</td></tr><tr><td>mean_reward</td><td>-0.47742</td></tr><tr><td>mean_reward_epoch_0</td><td>-0.52681</td></tr><tr><td>mean_reward_epoch_1</td><td>1.0</td></tr><tr><td>mean_reward_epoch_10</td><td>-0.44573</td></tr><tr><td>mean_reward_epoch_11</td><td>-0.42374</td></tr><tr><td>mean_reward_epoch_12</td><td>-0.25487</td></tr><tr><td>mean_reward_epoch_13</td><td>-0.29401</td></tr><tr><td>mean_reward_epoch_14</td><td>-0.62808</td></tr><tr><td>mean_reward_epoch_15</td><td>-0.55621</td></tr><tr><td>mean_reward_epoch_16</td><td>-0.37885</td></tr><tr><td>mean_reward_epoch_17</td><td>-0.72348</td></tr><tr><td>mean_reward_epoch_18</td><td>-0.64645</td></tr><tr><td>mean_reward_epoch_19</td><td>-0.30718</td></tr><tr><td>mean_reward_epoch_2</td><td>1.0</td></tr><tr><td>mean_reward_epoch_20</td><td>-0.46619</td></tr><tr><td>mean_reward_epoch_21</td><td>-0.75692</td></tr><tr><td>mean_reward_epoch_22</td><td>-0.46998</td></tr><tr><td>mean_reward_epoch_23</td><td>-0.50819</td></tr><tr><td>mean_reward_epoch_24</td><td>-0.77428</td></tr><tr><td>mean_reward_epoch_25</td><td>-0.67495</td></tr><tr><td>mean_reward_epoch_26</td><td>-0.67253</td></tr><tr><td>mean_reward_epoch_27</td><td>-0.52957</td></tr><tr><td>mean_reward_epoch_28</td><td>-1.0</td></tr><tr><td>mean_reward_epoch_29</td><td>-0.43535</td></tr><tr><td>mean_reward_epoch_3</td><td>-0.68116</td></tr><tr><td>mean_reward_epoch_30</td><td>-0.94228</td></tr><tr><td>mean_reward_epoch_31</td><td>-0.51522</td></tr><tr><td>mean_reward_epoch_32</td><td>-0.79548</td></tr><tr><td>mean_reward_epoch_33</td><td>-1.0</td></tr><tr><td>mean_reward_epoch_34</td><td>-0.7772</td></tr><tr><td>mean_reward_epoch_35</td><td>-0.36708</td></tr><tr><td>mean_reward_epoch_36</td><td>-0.71935</td></tr><tr><td>mean_reward_epoch_37</td><td>-0.67178</td></tr><tr><td>mean_reward_epoch_38</td><td>-0.61862</td></tr><tr><td>mean_reward_epoch_39</td><td>-1.0</td></tr><tr><td>mean_reward_epoch_4</td><td>-0.66671</td></tr><tr><td>mean_reward_epoch_40</td><td>-1.02667</td></tr><tr><td>mean_reward_epoch_41</td><td>-0.56453</td></tr><tr><td>mean_reward_epoch_42</td><td>-0.67482</td></tr><tr><td>mean_reward_epoch_43</td><td>-0.60699</td></tr><tr><td>mean_reward_epoch_44</td><td>-0.56651</td></tr><tr><td>mean_reward_epoch_45</td><td>-0.70495</td></tr><tr><td>mean_reward_epoch_46</td><td>-0.60896</td></tr><tr><td>mean_reward_epoch_47</td><td>-0.6068</td></tr><tr><td>mean_reward_epoch_48</td><td>-0.50121</td></tr><tr><td>mean_reward_epoch_49</td><td>-0.23096</td></tr><tr><td>mean_reward_epoch_5</td><td>-0.775</td></tr><tr><td>mean_reward_epoch_50</td><td>-0.46014</td></tr><tr><td>mean_reward_epoch_51</td><td>-0.60587</td></tr><tr><td>mean_reward_epoch_52</td><td>-0.41152</td></tr><tr><td>mean_reward_epoch_53</td><td>-0.24919</td></tr><tr><td>mean_reward_epoch_54</td><td>-1.0</td></tr><tr><td>mean_reward_epoch_55</td><td>-0.58288</td></tr><tr><td>mean_reward_epoch_56</td><td>-0.50058</td></tr><tr><td>mean_reward_epoch_57</td><td>-0.49467</td></tr><tr><td>mean_reward_epoch_58</td><td>-0.4261</td></tr><tr><td>mean_reward_epoch_59</td><td>-0.50135</td></tr><tr><td>mean_reward_epoch_6</td><td>-0.89791</td></tr><tr><td>mean_reward_epoch_60</td><td>-0.572</td></tr><tr><td>mean_reward_epoch_61</td><td>-0.28507</td></tr><tr><td>mean_reward_epoch_62</td><td>-0.38668</td></tr><tr><td>mean_reward_epoch_63</td><td>-0.54562</td></tr><tr><td>mean_reward_epoch_64</td><td>-0.40862</td></tr><tr><td>mean_reward_epoch_65</td><td>-0.38122</td></tr><tr><td>mean_reward_epoch_66</td><td>-0.25622</td></tr><tr><td>mean_reward_epoch_67</td><td>-1.0</td></tr><tr><td>mean_reward_epoch_68</td><td>-0.49256</td></tr><tr><td>mean_reward_epoch_69</td><td>-0.48915</td></tr><tr><td>mean_reward_epoch_7</td><td>-0.90199</td></tr><tr><td>mean_reward_epoch_70</td><td>-0.84732</td></tr><tr><td>mean_reward_epoch_71</td><td>-0.32499</td></tr><tr><td>mean_reward_epoch_72</td><td>-0.42061</td></tr><tr><td>mean_reward_epoch_73</td><td>1.0</td></tr><tr><td>mean_reward_epoch_74</td><td>-0.41545</td></tr><tr><td>mean_reward_epoch_75</td><td>-0.38611</td></tr><tr><td>mean_reward_epoch_76</td><td>-0.32948</td></tr><tr><td>mean_reward_epoch_77</td><td>-0.24056</td></tr><tr><td>mean_reward_epoch_78</td><td>-0.89323</td></tr><tr><td>mean_reward_epoch_79</td><td>-0.84076</td></tr><tr><td>mean_reward_epoch_8</td><td>-1.0</td></tr><tr><td>mean_reward_epoch_80</td><td>-0.76384</td></tr><tr><td>mean_reward_epoch_81</td><td>-0.65239</td></tr><tr><td>mean_reward_epoch_82</td><td>-0.53019</td></tr><tr><td>mean_reward_epoch_83</td><td>-0.96767</td></tr><tr><td>mean_reward_epoch_84</td><td>-0.26591</td></tr><tr><td>mean_reward_epoch_85</td><td>1.0</td></tr><tr><td>mean_reward_epoch_86</td><td>-0.42155</td></tr><tr><td>mean_reward_epoch_87</td><td>-0.54489</td></tr><tr><td>mean_reward_epoch_88</td><td>-0.37533</td></tr><tr><td>mean_reward_epoch_89</td><td>-0.26315</td></tr><tr><td>mean_reward_epoch_9</td><td>-0.4381</td></tr><tr><td>mean_reward_epoch_90</td><td>-0.5048</td></tr><tr><td>mean_reward_epoch_91</td><td>-0.50998</td></tr><tr><td>mean_reward_epoch_92</td><td>-0.61204</td></tr><tr><td>mean_reward_epoch_93</td><td>-0.5241</td></tr><tr><td>mean_reward_epoch_94</td><td>-0.38366</td></tr><tr><td>mean_reward_epoch_95</td><td>-0.66762</td></tr><tr><td>mean_reward_epoch_96</td><td>-0.37661</td></tr><tr><td>mean_reward_epoch_97</td><td>-0.41651</td></tr><tr><td>mean_reward_epoch_98</td><td>-0.33985</td></tr><tr><td>mean_reward_epoch_99</td><td>-0.47742</td></tr><tr><td>reward_agent_0_env_0</td><td>-0.47742</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">youthful-blaze-55</strong> at: <a href='https://wandb.ai/filocava99/vmas/runs/524c8emc' target=\"_blank\">https://wandb.ai/filocava99/vmas/runs/524c8emc</a><br/> View job at <a href='https://wandb.ai/filocava99/vmas/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk2OTc0Njk2/version_details/v21' target=\"_blank\">https://wandb.ai/filocava99/vmas/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjk2OTc0Njk2/version_details/v21</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20230917_143416-524c8emc/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:524c8emc). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.15.10 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.8"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/filippocavallari/DataspellProjects/deeplearning/wandb/run-20230917_170545-uajiwclg</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/filocava99/vmas/runs/uajiwclg' target=\"_blank\">lucky-galaxy-56</a></strong> to <a href='https://wandb.ai/filocava99/vmas' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/filocava99/vmas' target=\"_blank\">https://wandb.ai/filocava99/vmas</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/filocava99/vmas/runs/uajiwclg' target=\"_blank\">https://wandb.ai/filocava99/vmas/runs/uajiwclg</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "Step 1\n",
      "Step 2\n",
      "Step 3\n",
      "Step 4\n",
      "Step 5\n",
      "Step 6\n",
      "Step 7\n",
      "Step 8\n",
      "Step 9\n",
      "Step 10\n",
      "Step 11\n",
      "Step 12\n",
      "Step 13\n",
      "Step 14\n",
      "Step 15\n",
      "Step 16\n",
      "Step 17\n",
      "Step 18\n",
      "Step 19\n",
      "Step 20\n",
      "Step 21\n",
      "Step 22\n",
      "Step 23\n",
      "Step 24\n",
      "Step 25\n",
      "Step 26\n",
      "Step 27\n",
      "Step 28\n",
      "Step 29\n",
      "Step 30\n",
      "Step 31\n",
      "Step 32\n",
      "Step 33\n",
      "Step 34\n",
      "Step 35\n",
      "Step 36\n",
      "Step 37\n",
      "Step 38\n",
      "Step 39\n",
      "Step 40\n",
      "Step 41\n",
      "Step 42\n",
      "Step 43\n",
      "Step 44\n",
      "Step 45\n",
      "Step 46\n",
      "Step 47\n",
      "Step 48\n",
      "Step 49\n",
      "Step 50\n",
      "Step 51\n",
      "Step 52\n",
      "Step 53\n",
      "Step 54\n",
      "Step 55\n",
      "Step 56\n",
      "Step 57\n",
      "Step 58\n",
      "Step 59\n",
      "Step 60\n",
      "Step 61\n",
      "Step 62\n",
      "Step 63\n",
      "Step 64\n",
      "Step 65\n",
      "Step 66\n",
      "Step 67\n",
      "Step 68\n",
      "Step 69\n",
      "Step 70\n",
      "Step 71\n",
      "Step 72\n",
      "Step 73\n",
      "Step 74\n",
      "Step 75\n",
      "Step 76\n",
      "Step 77\n",
      "Step 78\n",
      "Step 79\n",
      "Step 80\n",
      "Step 81\n",
      "Step 82\n",
      "Step 83\n",
      "Step 84\n",
      "Step 85\n",
      "Step 86\n",
      "Step 87\n",
      "Step 88\n",
      "Step 89\n",
      "Step 90\n",
      "Step 91\n",
      "Step 92\n",
      "Step 93\n",
      "Step 94\n",
      "Step 95\n",
      "Step 96\n",
      "Step 97\n",
      "Step 98\n",
      "Step 99\n",
      "Step 100\n",
      "Step 101\n",
      "Step 102\n",
      "Step 103\n",
      "Step 104\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import threading\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import tensor, Tensor\n",
    "from vmas import make_env\n",
    "\n",
    "import wandb\n",
    "from Cleaning import Scenario as CleaningScenario\n",
    "from DeepQLearner import DeepQLearner\n",
    "from LearningConfiguration import LearningConfiguration, NNFactory\n",
    "from ReplayBuffer import ReplayBufferFactory\n",
    "import Device\n",
    "\n",
    "scenario_name = CleaningScenario()\n",
    "\n",
    "# Scenario specific variables\n",
    "n_agents = 1\n",
    "n_targets = 8\n",
    "num_envs = 1  # Number of vectorized environments\n",
    "continuous_actions = True\n",
    "device = Device.get()  # or cuda or any other torch device\n",
    "n_steps = 1000  # Number of steps before returning done\n",
    "n_epochs = 1000\n",
    "dict_spaces = True  # Weather to return obs, rewards, and infos as dictionaries with agent names (by default they are lists of len # of agents)\n",
    "\n",
    "run = wandb.init(project=\"vmas\", reinit=True, config={\n",
    "    \"learning_rate\": 0.0005,\n",
    "    \"architecture\": \"MLP\",\n",
    "    #\"epochs\": n_steps\n",
    "})\n",
    "\n",
    "dataset_size = 10000\n",
    "\n",
    "frame_list = []  # For creating a gif\n",
    "init_time = time.time()\n",
    "step = 0\n",
    "\n",
    "# Actions\n",
    "speed = 0.5\n",
    "north = tensor([0, -1*speed])\n",
    "south = tensor([0, speed])\n",
    "east = tensor([speed, 0])\n",
    "west = tensor([-1*speed, 0])\n",
    "#stop = tensor([0, 0])\n",
    "ne = tensor([speed, -1*speed])\n",
    "nw = tensor([-1*speed, -1*speed])\n",
    "se = tensor([speed, speed])\n",
    "sw = tensor([-1*speed, speed])\n",
    "\n",
    "lidar_measure_shape = 50 * 2\n",
    "pos_shape = 2\n",
    "vel_shape = 2\n",
    "tot_shape = lidar_measure_shape + pos_shape + vel_shape\n",
    "\n",
    "actions = [north, south, east, west, ne, nw, se, sw]\n",
    "#learning_configuration = LearningConfiguration(update_each=math.floor(n_steps/3),dqn_factory=NNFactory(tot_shape,64,len(actions)))\n",
    "learning_configuration = LearningConfiguration(update_each=499,dqn_factory=NNFactory(tot_shape,64,len(actions)))\n",
    "\n",
    "dql = DeepQLearner(\n",
    "    memory=ReplayBufferFactory(dataset_size),\n",
    "    action_space=actions,\n",
    "    learning_configuration=learning_configuration\n",
    ")\n",
    "\n",
    "targets_pos = []\n",
    "\n",
    "for i in range(n_targets):\n",
    "    targets_pos.append(tensor([random.random() * random.randint(-1, 1), random.random() * random.randint(-1, 1)], device=Device.get()))    \n",
    "\n",
    "def isOneEnvDone(info_array):\n",
    "    tensor = info_array[\"agent_0\"][\"active_targets\"]\n",
    "    for i in range(num_envs):\n",
    "        if tensor[i] == 0:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def save_gif(frame_list, epoch):\n",
    "    for i in range(1):\n",
    "        gif_name = scenario_name.__class__.__name__ + \"-env-\" + str(i) + \"-epoch-\" + str(epoch) + \".gif\"\n",
    "        frame_list[i].save(\n",
    "                gif_name,\n",
    "                save_all=True,\n",
    "                append_images=frame_list[1:],\n",
    "                duration=1,\n",
    "                loop=0,\n",
    "            )\n",
    "    dql.snapshot(epoch, \"0\")\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    env = make_env(\n",
    "        scenario=scenario_name,\n",
    "        num_envs=num_envs,\n",
    "        device=device,\n",
    "        continuous_actions=continuous_actions,\n",
    "        dict_spaces=dict_spaces,\n",
    "        wrapper=None,\n",
    "        seed=None,\n",
    "        n_targets=n_targets,\n",
    "        n_agents=n_agents,\n",
    "        wandb=wandb,\n",
    "        targets_pos=targets_pos\n",
    "    )\n",
    "    previous_states = {}\n",
    "    for step in range(n_steps):\n",
    "        print(f\"Step {step}\")\n",
    "        actions = {}\n",
    "        logs = {}\n",
    "        for i, agent in enumerate(env.agents):\n",
    "            lidar_measure = previous_states[agent.name][\"lidar_measure\"] if step > 0 else torch.zeros(num_envs, lidar_measure_shape).to(Device.get())\n",
    "            positions = agent.state.pos\n",
    "            velocities = agent.state.vel\n",
    "            agent_actions_list = []\n",
    "            for j in range(num_envs):\n",
    "                state = torch.cat((positions[j], velocities[j], lidar_measure[j]),dim=-1).to(Device.get())\n",
    "                action = dql.behavioural(state)\n",
    "                #print(action)\n",
    "                agent_actions_list.append(action)\n",
    "            agent_actions = torch.stack(agent_actions_list)\n",
    "            actions.update({agent.name: agent_actions})\n",
    "            if step > dql.batch_size/num_envs:\n",
    "                dql.improve() # Improve the model\n",
    "                #TODO Should I do the improve once for each env or once for each agent?\n",
    "        obs, rewards, dones, info = env.step(actions)\n",
    "        mean_reward = 0\n",
    "        #print(rewards)\n",
    "        for i, agent in enumerate(env.agents):\n",
    "            positions = agent.state.pos\n",
    "            velocities = agent.state.vel\n",
    "            lidar_measure = obs[agent.name][:, (tot_shape - lidar_measure_shape):]\n",
    "            previous_states.update({agent.name: {\"lidar_measure\": lidar_measure, \"pos\": positions, \"vel\": velocities}})\n",
    "            for j in range(num_envs):\n",
    "                reward = rewards[agent.name][j]\n",
    "                mean_reward += reward\n",
    "                logs.update({f\"reward_{agent.name}_env_{j}\": reward})\n",
    "                prev_state = previous_states[agent.name]\n",
    "                prev_state = torch.cat((prev_state[\"pos\"][j], prev_state[\"vel\"][j], prev_state[\"lidar_measure\"][j]),dim=-1).to(Device.get())\n",
    "                state = obs[agent.name][j]\n",
    "                action = actions[agent.name][j]\n",
    "                dql.record(prev_state,action,reward,state)\n",
    "        mean_reward /= (num_envs*n_agents)\n",
    "        logs.update({\"epsilon\": dql.epsilon.value()})\n",
    "        logs.update({\"loss\": dql.last_loss})\n",
    "        logs.update({\"mean_reward\": mean_reward})\n",
    "        logs.update({f\"mean_reward_epoch_{e}\": mean_reward})\n",
    "    \n",
    "        wandb.log(logs)\n",
    "        dql.epsilon.update() # Update epsilon\n",
    "        #dql.snapshot(step, \"0\")\n",
    "        frame_list.append(\n",
    "            Image.fromarray(env.render(mode=\"rgb_array\", agent_index_focus=None))\n",
    "        )  # Can give the camera an agent index to focus on\n",
    "        \n",
    "        #print(info)\n",
    "        if isOneEnvDone(info):\n",
    "            print(\"Env done\")\n",
    "            dql.target_network.load_state_dict(dql.policy_network.state_dict())\n",
    "            break\n",
    "    \n",
    "    \n",
    "    # Produce a gif\n",
    "    frame_list_copy = copy.deepcopy(frame_list)\n",
    "    thread = threading.Thread(target=save_gif, args=(frame_list_copy, e))\n",
    "    \n",
    "    thread.start()\n",
    "    \n",
    "    frame_list.clear()\n",
    "    \n",
    "    total_time = time.time() - init_time\n",
    "    print(\n",
    "        f\"It took: {total_time}s for {n_steps} steps of {num_envs} parallel environments on device {device} \"\n",
    "        f\"for {scenario_name} scenario.\"\n",
    "    )\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-09-17T15:05:45.395959Z"
    }
   },
   "id": "e36043d3bc405e42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bd54fe5a0943c71a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
